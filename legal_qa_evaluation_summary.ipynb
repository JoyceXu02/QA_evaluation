{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6d94063-3405-401e-afa7-d8443b2c6760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fb98f0e-cd12-4219-824a-a5b2033e0ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read OpenAI key\n",
    "key = open('openai_key.txt', 'r').read()\n",
    "os.environ[\"OPENAI_API_KEY\"] = key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb791d-8494-4e71-b910-7191db3997a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GPT4 Question-Answer Pairs Generation (Ground Truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6ca5586-95a8-4f69-b4fc-54a4d3e54af3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summ_test = pd.read_csv(\"summ_test.csv\", delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c657eca-8c2c-4f00-bdd8-dbe6b4f07d10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_annotation(row):\n",
    "    if row[\"IRC_type\"] != \"Non_IRC\":\n",
    "        left_label = \"<\"+row[\"IRC_type\"]+\">\"\n",
    "        right_label = \"</\"+row[\"IRC_type\"]+\">\"\n",
    "        annotated_sent = left_label + row[\"sentence\"] + right_label\n",
    "    else:\n",
    "        annotated_sent = row[\"sentence\"]\n",
    "    return annotated_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6906d35-5f64-4437-b9a2-4508021e2902",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summ_test[\"annotated_sentence\"] = summ_test.apply(lambda x: text_annotation(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2679fe0b-e87a-4a2b-839a-c055ad0f3a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summ_test_grouped = summ_test.groupby(\"name\")[[\"sentence\", \"annotated_sentence\"]].agg(lambda x: \" \".join(x)).reset_index().rename(columns = {\"sentence\": \"summary\", \"annotated_sentence\": \"annotated_summary\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77e75874-e992-4fce-98e7-2aa76b731732",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_prompt = \"\"\"\n",
    "    Act like a legal professional and read the following legal text that delimitered by three backticks. Use Issue, Reason and Conclusion sentences to generate question-answer pairs.\n",
    "\n",
    "    List those generated questions and answers in the following format and put it in a json format:\n",
    "\n",
    "    Question1:...\n",
    "    Type1:...\n",
    "    Answer1:...\n",
    "\n",
    "    Question2:...\n",
    "    Type2:...\n",
    "    Answer2:...\n",
    "\n",
    "    Question3:...\n",
    "    Type3:...\n",
    "    Answer3:...\n",
    "\n",
    "    ```%s```\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34ee5416-3a73-4d55-899b-c7e72c62cf54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def qa_generation(prompt_template, summary, model='gpt-4'):\n",
    "    qa_prompt = prompt_template %summary\n",
    "    messages = [{\"role\": \"user\", \"content\": qa_prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ce6a171-85c6-4537-b51b-d8c60e79956d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "name                                                1987canlii2373.txt\n",
      "summary              Warrant issued to search a dwelling house for ...\n",
      "annotated_summary    <Issue>Warrant issued to search a dwelling hou...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "qs = []\n",
    "ans = []\n",
    "q_types = []\n",
    "names = []\n",
    "for idx, row in summ_test_grouped[:15].iterrows():\n",
    "    summary = row[\"annotated_summary\"]\n",
    "    name = row[\"name\"]\n",
    "    response = qa_generation(qa_prompt, summary)\n",
    "    literal_response = ast.literal_eval(response)\n",
    "    names.append(name)\n",
    "    for key,value in literal_response.items():        \n",
    "        if \"Question\" in key:\n",
    "            qs.append(qa[key])\n",
    "        elif \"Answer\" in key:\n",
    "            ans.append(qa[key])\n",
    "        else:\n",
    "            q_types.append(qa[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33917dba-6cdc-4cde-bdc2-45fb4bb733fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GPT4 Answer Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "345f74fc-0346-40bb-8fa9-fe080becf119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c168540-e1e8-44e9-9354-c0bf226fc3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name = 'gpt-4',temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e6a92-ebc2-4eaf-b7b7-53a80b35fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your own generated legal summary (csv format)\n",
    "generated_summaries = pd.read_csv(\"...csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b2031f-1fb0-4285-9ace-51b585290225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(llm, context, q, ans, ttype, name):\n",
    "    eval_chain = QAEvalChain.from_llm(llm)\n",
    "    context_examples = [{\"question\": q, \"context\": context, \"type\": ttype}]\n",
    "    QA_PROMPT = \"Answer the {type} type of question based on the context\\nContext:{context}\\nQuestion:{question}\\nAnswer:\"\n",
    "    template = PromptTemplate(input_variables=[\"type\", \"context\", \"question\"], template=QA_PROMPT)\n",
    "    qa_chain = LLMChain(llm=llm, prompt=template)\n",
    "    pred = qa_chain.apply(context_examples)\n",
    "    return pred[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae35537b-1bc2-4946-b9d6-e60616a4f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i, row in generated_summaries.iterrows():\n",
    "    name = row['name']\n",
    "    q = row['question']\n",
    "    ans = row['ground_truth_answer']\n",
    "    ttype = row['IRC_type']\n",
    "    context = row['model_summary']\n",
    "    pred = predict_answer(llm, context, q, ans, ttype, name)\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82569f6e-93bf-4fd5-b1d6-5c152356b070",
   "metadata": {},
   "source": [
    "### GPT4 Answer Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af98e5b9-4ad5-4f09-b1d8-4a319d233463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EVAL_PROMPT = \"\"\"You are a legal expert to judge the answers to questions.\n",
    "You are judging the following question:\n",
    "{query}\n",
    "Here is the real answer:\n",
    "{answer}\n",
    "You are grading the following predicted answer:\n",
    "{result}\n",
    "What grade do you give from 0 to 10, where 0 is the lowest (can't find the answer) and 10 is the highest (very close to the real answer)?\n",
    "\n",
    "Finally, give then explain the reason of the grade in the following format:\n",
    "Explanation: <explain>.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b4a01da-f105-4e7d-84da-ece22d6b335f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_answer(llm, prompt_template, qs, ans, types, names, preds):\n",
    "    examples = [{\"question\": qs[i], \"answer\": preds[i]['text']} for i in range(len(qs))]\n",
    "    ans = [{'text': a} for a in ans]\n",
    "    \n",
    "    PROMPT = PromptTemplate(input_variables=[\"query\", \"answer\", \"result\"], template=prompt_template)    \n",
    "    eval_chain = QAEvalChain.from_llm(llm,prompt=PROMPT)\n",
    "    graded_outputs = eval_chain.evaluate(examples, ans, question_key=\"question\", prediction_key=\"text\")\n",
    "\n",
    "    questions, real_answers, summary_answers, graded = [], [], [], []\n",
    "    for i, eg in enumerate(examples):\n",
    "        questions.append(eg['question'])\n",
    "        real_answers.append(eg['answer'])\n",
    "        summary_answers.append(ans[i]['text'])\n",
    "        graded.append(graded_outputs[i]['text'])\n",
    "    returned_dict = {\"name\": names,\n",
    "                     \"question\": questions, \n",
    "                     \"IRC_type\": types,\n",
    "                     \"real_answer\": real_answers, \n",
    "                     \"summary_answer\": summary_answers,\n",
    "                     \"grade_output\": graded\n",
    "                     }\n",
    "    return returned_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50974aca-3b99-48f1-8150-5d54d9ac0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# post-process the returned result\n",
    "res_dict = evalute_answer(llm, EVAL_PROMPT, qs, ans, types, names, preds)\n",
    "df = pd.DataFrame(res_dict)   \n",
    "df['grade_output'] = df['grade_output'].apply(lambda x: re.split(r'\\n{1,2}', x))    \n",
    "df['grade'] = df['grade_output'].apply(lambda x: x[0])\n",
    "df['explanation'] = df['grade_output'].apply(lambda x: x[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
